## PT 면접 준비

참고: https://brunch.co.kr/@happinessdanish/364

```text
R - S - T - A 방식 (프로젝트 경험을 말할 때)

R: Result (결론) – 먼저 프로젝트나 경험의 결과를 말해라.
S: Situation (상황) – 그 결과가 나온 상황을 설명해라.
T: Task (과제) – 그 상황에서 맡았던 역할이나 과제를 말해라.
A: Action (행동) – 그 과제를 해결하기 위해 어떤 행동을 했는지 구체적으로 설명해라.
```

### BookChallenge 8분 스크립트

```text
<독서 동기를 높이는 경쟁 기반 독서 챌린지 서비스> 프로젝트에서 
<페이징 API의 TPS를 4.3에서 245>로 향상시킨 경험이 있습니다.


<임시 테스트로 넣은 500만 건의 도서>에 대해 <정렬조건>에 따라 책을 조회하는 <페이징 API>의 <성능이 매우 낮았습니다.>
<기존 페이징 API의 TPS>는 <4.3>으로, primary key로 인덱스가 설정된 <책 ID로 조회 시> 기록된 <TPS 230에 비해> 현저히 <낮은 수준>이었습니다.

따라서 <페이징 API의 성능을 개선>하여 <목표 TPS 200>을 달성해야 했습니다.

개선하기위해 
먼저, AWS EC2에서 NCloud로 <서버를 이전>하여 <리소스를 확장>했지만, TPS 유의미한 개선이 없음을 확인했습니다.
이는 각 트랜잭션이 사용하는 CPU 사용량이 0.05프로, 메모리 사용량을 5MB로 가정했을 때 TPS 200의 목표를 도달하기 위해서는 충분히 1CPU 1GB 램으로도 가능하다는 사실을 알게됐습니다. 

페이징 API에서 발생하는 <페이징 쿼리>에 대해 <EXPLAIN> 명령어로 쿼리 <실행 계획>을 분석하여 <테이블 풀 스캔과 filesort>로 인한 성능 저하를 발견했습니다.
따라서 <정렬조건 칼럼>인 created_at, book_name 등에 대해 <단일 인덱스>를 생성하여 성능을 일부 개선했습니다.
하지만 <OFFSET>이 300만이 <넘어갈 수록> 실행시간이 2.6초로 증가하고, 옵티마이저가 filesort로 다시 판단했습니다.

그 이유는 OFFSET은 10개의 데이터만 조회한다고 해도 앞 300만 건의 데이터도 모두 읽습니다. 
즉 300만 + 10개의 데이터를 <순차적으로 읽어들이고> 그중 필요없는 <300만건의 데이터는 버리고> 10건만 보냅니다. 
또한 이 300만건은 created_at의 인덱스 테이블에서만 읽는게 아니라 <다른 칼럼도 읽기 위해 다시 테이블로 가서> 데이터를 읽기 때문에 시간이 상당히 증가했습니다.

따라서 필요한 모든 컬럼을 포함하는 <커버링 인덱스를 생성>하여 쿼리 실행 시간을 줄였습니다.
하지만 인덱스 <관리의 복잡성>과 <쓰기 작업의 성능 저하>로 다른 방법을 찾았습니다.

이런 OFFSET 방식의 한계를 극복하기 위해 <WHERE 절을 활용한 NO-OFFSET 방식>을 적용했습니다.
이로써 인덱스풀스캔에서 <인덱스 레인지 스캔>을 활용하게 되어 쿼리 실행 시간이 대폭 감소했고, TPS는 245로 향상되었습니다.

이렇게 인덱스 생성과 No-OFFSET 방식의 변경으로 성능을 대폭 개선했지만, 비즈니스 상황에 따라 무한 스크롤 방식이 불가능 경우도 있기 때문에 추가적인 개선점을 고려했습니다.

책 검색 API에서도 비슷한 요구사항인 페이징 처리가 필요했었는데, 이때는 OFFSET 방식으로 개선해보기로 정했습니다.

책 검색 API에서는 다양한 조건이 들어갑니다. 조건절에 여러개의 조건이 들어가면 성능이 급격히 안좋아지는 현상이 발생했습니다.

테이블 풀 스캔에 duration은 5.364 초 입니다. where 절의 책이름과 페이지수 칼럼 각각에 이미 인덱스를 생성한 상태에도 불구하고, 테이블 풀 스캔으로 진행되었습니다. 
당연하게도 조건이 한개일때는 적용되지만, 두개가 같이 들어가니 인덱스 테이블을 더이상 타지 않았습니다.

주로 책 이름과 페이지 카운트만 검색한다고 가정하고, 복합인덱스를 생성할 때 자주 검색되는 조건인 책 이름 칼럼을 앞 순위에 배치하여 생성하여 성능을 5초에서 1초대로 개선했습니다.
또한, 책이름, 페이지카운트 순서인 인덱스 테이블보다는 페이지 카운트, 책이름 순서인 인덱스 테이블이 필터링을 더 많이 거칠거라 예상했고 인덱스 순서를 변경하여 재생성한 결과 1초대에서 약 0.3초로 크게 개선되었습니다.

여기서 카운트 캐시를 적용하는 방식으로도 개선할 방법이 있지만, 검색 API에서는 검색 조건에 따라 시시각각 바뀌기 때문에 불가능합니다. 
또한, 카운트 테이블을 생성하는 방식도 같은 이유로 불가능합니다.

그래서 생각해보니 애초에 카운트 쿼리가 문제일 것이라고 생각이 들었기 때문에 한 가지 가정을 했습니다. 
대부분의 요청이 검색 버튼을 클릭하고, 페이지 버튼을 통한 조회 요청이 거의 없을 경우가 많을거라고 가정했습니다.

이럴 경우 검색 버튼을 클릭한 경우에만 Page 수를 고정하고 (카운트 쿼리는 발생하지 않게 하고), 
다음 페이지로 이동하기 위해 페이지 버튼을 클릭했을 때만 실제 페이지 count 쿼리를 발생시켜 정확한 페이지 수를 사용하게 하면 됩니다.

따라서 코드를 구현할 때 검색버튼 클릭 여부가 true 일 때 고정된 페이징 토탈 건수를 반환하고, 페이지버튼을 눌렀을 때는 카운트쿼리를 발생하는 식으로 수행했습니다.
하지만 UX상에서 동적으로 페이지 번호가 바뀌는 상황이 불가능하거나 검색버튼 보다 페이지 번호를 누르는 일이 많으면 이 방식 역시 좋은 방식은 아닙니다. 
그때는 첫 페이지만 카운트쿼리를 하고, 그 후엔 프론트 영역에서 카운트를 캐싱하고 보내주는 방식으로 해결할 수 있을 것 같습니다.  
```

### AgileHub 8분 스크립트

```text
대학생들이 Jira의 복잡한 기능과 UI로 인해 어려움을 겪는다는 점을 고려하여, 직관적인 인터페이스와 단계별 가이드를 통해 누구나 쉽게 접근할 수 있도록 기획했습니다.  
저는 대표적인 기능들인 백로그관리와 스프린트관리, 이슈관리 등을 맡아 API 개발과 인프라 구성을 했습니다. 

해당 프로젝트에서 이슈 조회 API의 성능을 개선하여, 
타임아웃 문제를 해결하고 쿼리 실행 시간을 기존 3.59초에서 1.25초로 약 65% 단축한 경험이 있습니다.

이 프로젝트에서 가장 큰 문제는 400만 건의 데이터를 처리하는 이슈 조회 API에서 발생한 심각한 성능 저하였습니다. 
조회 로직이 비효율적으로 설계되어 데이터 호출 시 여러 쿼리가 동시에 실행되었고, 이로 인해 JDBC 타임아웃이 빈번하게 발생하였습니다.

기존에는 프로젝트에 할당된 모든 이슈들을 관계가 복잡하게 맺어진 채로 한 번에 가져오는 API 구조였는데, 이로 인해 시간 복잡도가 O(N^3)에 달할 정도로 비효율적이었습니다.

이를 해결하기 위해 API를 세 가지로 분리하였습니다

프로젝트에 할당된 에픽만 가져오는 API
특정 에픽 ID를 부모로 하여 관련 스토리들을 가져오는 API
특정 스토리 ID를 부모로 하여 관련 태스크들을 가져오는 API

이렇게 API를 분리한 후, IN 절을 사용하여 다중 쿼리 요청을 단일 요청으로 최적화하였습니다. 이를 통해 더 이상 JDBC 타임아웃이 발생하지 않게 되었습니다.

다음 단계로는, 쿼리 성능을 최적화하기 위해 EXPLAIN ANALYZE를 사용하여 쿼리 실행 계획을 분석했습니다.

분석 결과, 스토리 테이블이 드라이빙 테이블로 설정되어 있어 데이터 누락 문제가 발생할 수 있다는 점을 발견하였습니다.
이를 해결하기 위해, 에픽 테이블이 드라이빙 테이블이 되도록 쿼리를 수정하였습니다. 이후, 쿼리 실행 시간을 단축하기 위해 필요한 칼럼만 추출하는 from절 서브쿼리를 사용하여 쿼리를 재작성했습니다.
그 결과, 쿼리 실행 시간이 기존 3.59초에서 1.25초로 약 65% 단축되었습니다.

마지막으로, Native Query와 Projection을 조합하는 방식을 도입했습니다.
성능과 가독성을 고려해 QueryDSL을 사용하려 했으나, FROM 절 서브쿼리를 지원하지 않는 제약을 발견했습니다.
이를 해결하기 위해 Native Query를 사용하면서 Projection 기법을 결합해 필요한 데이터를 효율적으로 가져오는 방식으로 최적화하였습니다. 이를 통해 QueryDSL의 제약을 우회하면서 성능을 개선할 수 있었습니다.



```

### AgileHub 4분 스크립트

```text
대학생들이 Jira의 복잡한 기능과 UI로 인해 어려움을 겪는다는 점을 고려하여, 직관적인 인터페이스와 단계별 가이드를 통해 누구나 쉽게 접근할 수 있도록 기획했습니다.  
저는 대표적인 기능들인 백로그관리와 스프린트관리, 이슈관리 등을 맡아 API 개발과 인프라 구성을 했습니다. 

해당 프로젝트에서 이슈 조회 API의 성능을 개선하여, 
타임아웃 문제를 해결하고 쿼리 실행 시간을 기존 3.59초에서 1.25초로 약 65% 단축한 경험이 있습니다.

이 프로젝트에서 가장 큰 문제는 400만 건의 데이터를 처리하는 이슈 조회 API에서 발생한 심각한 성능 저하였습니다. 
조회 로직이 비효율적으로 설계되어 데이터 호출 시 여러 쿼리가 동시에 실행되었고, 이로 인해 JDBC 타임아웃이 빈번하게 발생하였습니다.

기존에는 프로젝트에 할당된 모든 이슈들을 관계가 복잡하게 맺어진 채로 한 번에 가져오는 API 구조였는데, 이로 인해 시간 복잡도가 O(N^3)에 달할 정도로 비효율적이었습니다.

이를 해결하기 위해 API를 세 가지로 분리하였습니다

프로젝트에 할당된 에픽만 가져오는 API
특정 에픽 ID를 부모로 하여 관련 스토리들을 가져오는 API
특정 스토리 ID를 부모로 하여 관련 태스크들을 가져오는 API

이렇게 API를 분리한 후, IN 절을 사용하여 다중 쿼리 요청을 단일 요청으로 최적화하였습니다. 이를 통해 더 이상 JDBC 타임아웃이 발생하지 않게 되었습니다.

다음 단계로는, 쿼리 성능을 최적화하기 위해 EXPLAIN ANALYZE를 사용하여 쿼리 실행 계획을 분석했습니다.

분석 결과, 스토리 테이블이 드라이빙 테이블로 설정되어 있어 데이터 누락 문제가 발생할 수 있다는 점을 발견하였습니다.
이를 해결하기 위해, 에픽 테이블이 드라이빙 테이블이 되도록 쿼리를 수정하였습니다. 이후, 쿼리 실행 시간을 단축하기 위해 필요한 칼럼만 추출하는 서브쿼리를 사용하여 쿼리를 재작성했습니다.
그 결과, 쿼리 실행 시간이 기존 3.59초에서 1.25초로 약 65% 단축되었습니다.

마지막으로, Native Query와 Projection을 조합하는 방식을 도입했습니다.
성능과 가독성을 고려해 QueryDSL을 사용하려 했으나, FROM 절 서브쿼리를 지원하지 않는 제약을 발견했습니다.
이를 해결하기 위해 Native Query를 사용하면서 Projection 기법을 결합해 필요한 데이터를 효율적으로 가져오는 방식으로 최적화하였습니다. 이를 통해 QueryDSL의 제약을 우회하면서 성능을 개선할 수 있었습니다.
```


###  BookChallenge 4분 스크립트

```text
<독서 동기를 높이는 경쟁 기반 독서 챌린지 서비스> 프로젝트에서 
<페이징 API의 TPS를 4.3에서 245>로 향상시킨 경험이 있습니다.


<임시 테스트로 넣은 500만 건의 도서>를 <정렬조건>에 따라 조회하는 <페이징 API>의 <성능이 매우 낮았습니다.>
<기존 페이징 API의 TPS>는 <4.3>으로, primary key로 인덱스가 설정된 <책 ID로 조회 시> 기록된 <TPS 230에 비해> 현저히 <낮은 수준>이었습니다.

따라서 <페이징 API의 성능을 개선>하여 <목표 TPS 200>을 달성해야 했습니다.

개선하기위해 
먼저, AWS EC2에서 NCloud로 <서버를 이전>하여 <리소스를 확장>했지만, TPS 유의미한 개선이 없음을 확인했습니다.

페이징 API에서 발생하는 <페이징 쿼리>에 대해 <EXPLAIN> 명령어로 쿼리 <실행 계획>을 분석하여 <테이블 풀 스캔과 filesort>로 인한 성능 저하를 발견했습니다.
따라서 <정렬조건 칼럼>인 created_at, book_name 등에 대해 <단일 인덱스>를 생성하여 성능을 일부 개선했습니다.
하지만 <OFFSET>이 300만이 <넘어갈 수록> 실행시간이 2.6초로 증가하고, 옵티마이저가 filesort로 다시 판단했습니다.

그 이유는 OFFSET은 10개의 데이터만 조회한다고 해도 앞 300만 건의 데이터도 모두 읽습니다. 
즉 300만 + 10개의 데이터를 <순차적으로 읽어들이고> 그중 필요없는 <300만건의 데이터는 버리고> 10건만 보냅니다. 
또한 이 300만건은 created_at의 인덱스 테이블에서만 읽는게 아니라 <다른 칼럼도 읽기 위해 다시 테이블로 가서> 데이터를 읽기 때문에 시간이 상당히 증가했습니다.

따라서 필요한 모든 컬럼을 포함하는 <커버링 인덱스를 생성>하여 쿼리 실행 시간을 줄였습니다.
하지만 인덱스 <관리의 복잡성>과 <쓰기 작업의 성능 저하>로 다른 방법을 찾았습니다.

이런 OFFSET 방식의 한계를 극복하기 위해 <WHERE 절을 활용한 NO-OFFSET 방식>을 적용했습니다.
이로써 인덱스풀스캔에서 <인덱스 레인지 스캔>을 활용하게 되어 쿼리 실행 시간이 대폭 감소했고, TPS는 245로 향상되었습니다.

이렇게 인덱스 생성과 No-OFFSET 방식의 변경으로 성능을 대폭 개선했지만, 이후에도 추가적인 쿼리 최적화와 다른 방식으로 더 개선할 수 있는 부분이 있었습니다. 
```