## PT

---

## 트러블 슈팅 (히카리 풀)

---

<details>
<summary><strong style="font-size:1.17em">
트러블 슈팅 경험이 있나요? 어떤 문제였고 어떻게 해결했나요? 
</strong></summary>

```text
대규모 조직에서 다수의 팀원들이 동시에 이슈를 등록하는 상황에 대비해 성능 테스트를 진행했습니다. 
1000명 규모 조직을 가정하고 100명의 동시 요청을 처리하되, 응답시간 2초 이내, 에러율 0.1% 이하를 목표로 설정했습니다.

테스트 중 가상 사용자(VUser)를 점진적으로 늘려가던 중, 20명 시점에서 문제가 발생했습니다. 
JPA 엔티티매니저가 트랜잭션을 생성하지 못하고 커넥션 타임아웃이 발생한 것입니다.

로그 분석 결과, 이슈 생성과 이슈 번호 발급 로직이 각각 별도의 커넥션을 사용하고 있었습니다. 
HikariCP의 기본 설정인 10개의 커넥션으로는 부족해 데드락이 발생하고 결국 타임아웃으로 이어진 것이었습니다.

문제 해결을 위해 다음과 같이 접근했습니다

기존에는 이슈 번호 생성 시 동시성 제어를 위해 비관적 락을 사용하고, 
성능 최적화를 위해 REQUIRED_NEW로 트랜잭션을 분리했었습니다.
하지만 이는 커넥션 풀 관리를 복잡하게 만들고 데드락을 유발했기에, 
먼저 분리된 트랜잭션을 하나로 통합했습니다.

이후 NGrinder를 활용한 부하 테스트를 통해 HikariCP와 톰캣 스레드 풀 크기를 최적화했습니다.

하지만 비관적 락이 이슈 생성 전체에 락이 걸렸기에 성능이 저하되었습니다.
이를 개선하기 위해 낙관적 락으로 전환하고, 
충돌 발생 시 최대 3회까지 재시도하는 커스텀 Retry 어노테이션을 구현했습니다.

하지만 예상과 달리, 동시 요청이 많은 상황에서 잦은 충돌과 
재시도로 인해 오히려 전체적인 성능이 더 저하되는 결과를 얻었습니다.

이를 해결하기 위해 최종적으로
이슈 번호 생성 로직을 Redis로 전환했습니다. Redis의 INCR 명령어를 활용하면 동시성을 보장하면서도 
데이터베이스 커넥션을 사용하지 않아 성능을 크게 개선할 수 있었습니다.

또한 잦은 쓰기 작업으로 인한 부하를 줄이기 위해 write-back 전략을 도입했습니다. 
Redis의 이슈 번호 데이터를 30초마다 스케줄러로 DB에 동기화하는 방식으로 구현했습니다.
그리고서 Redis가 갑자기 장애가 발생해 내려가는 문제를 방지하기 위해, 리플리케이션을 구성해 가용성을 확보했습니다.

최종적으로 100 VUser 기준 1분간 부하테스트 결과, TPS가 33에서 68로 증가하고 
평균 응답시간이 2.9초에서 1.4초로 개선되어 당초 목표였던 응답시간 2초 이내를 달성할 수 있었습니다.
```

</details>

---

<details>
<summary><strong style="font-size:1.17em">
HikariCP와 톰캣 스레드 풀 크기를 최적화를 왜 했나요?
</strong></summary>

```text
데이터베이스 커넥션과 애플리케이션 서버의 스레드는 모두 제한된 리소스이기 때문에, 
실제 동시 접속자 수에 맞게 적절한 크기를 설정하는 것이 중요했습니다.

너무 적게 설정하면 앞서 겪었던 것처럼 타임아웃이 발생하고, 
너무 많이 설정하면 서버 리소스가 불필요하게 낭비되거나 
오히려 컨텍스트 스위칭 오버헤드로 인해 성능이 저하될 수 있습니다.

그래서 NGrinder로 실제 부하 상황을 시뮬레이션하면서 최적의 설정값을 찾아냈습니다. 
가상 사용자를 점진적으로 늘려가며 응답시간과 에러율을 살펴보고 가장 안정적인 성능을
보이는 지점의 값을 선택했습니다. 
```

</details>


---

<details>
<summary><strong style="font-size:1.17em">
재시도 횟수는 어떤 기준으로 3회로 정했나요?
</strong></summary>

```text
재시도 횟수는 테스트를 통해 결정했습니다. 
10개의 스레드로 동시 요청을 발생시키는 테스트 코드를 작성해 측정해보니, 
3회 재시도 시 약 800ms가 소요되었습니다. 

이 결과를 바탕으로 목표로 하는 100명의 동시 접속 상황에서는 
재시도로 인한 성능 저하가 더욱 심각할 것으로 판단했습니다.
```

</details>

---

<details>
<summary><strong style="font-size:1.17em">
Redis를 선택한 이유는 무엇인가요?
</strong></summary>

```text
먼저 인메모리 DB이기 때문에 디스크 I/O가 발생하지 않아 매우 빠른 응답 속도를 보장할 수 있었습니다.

Redis의 INCR 명령어가 단일 스레드로 동작하고 원자성을 보장하기 때문에, 별도의 락 처리 없이도 동시성 문제를 해결할 수 있었습니다.

마지막으로 Redis는 이미 우리 시스템에서 캐시용도로 사용 중이었기 때문에, 추가적인 인프라 구축 없이 도입이 가능했습니다.
```

</details>

---

<details>
<summary><strong style="font-size:1.17em">
동기화 주기를 30초로 정한 이유가 있나요?
</strong></summary>

```text
동기화 주기는 서비스의 데이터 특성과 사용 패턴을 고려해서 30초로 정했습니다.

Redis에서 생성된 순수 이슈 번호는 즉시 'PRJ-123'과 같은 형태로 가공되어 별도 테이블에 저장되고, 
실제 서비스에서는 이 가공된 형태로만 조회됩니다. 

따라서 Redis의 원본 번호가 RDB와 일시적으로 동기화가 안 되어도 서비스 동작에는 전혀 영향이 없었습니다.

이런 구조적 특성 덕분에 동기화 주기를 너무 짧게 가져갈 필요가 없었고, 
30초 정도면 시스템 부하와 리소스 사용 측면에서 적절한 밸런스를 보여줬습니다
```

</details>

---

<details>
<summary><strong style="font-size:1.17em">
Redis 리플리케이션 구성은 어떻게 하셨나요?
</strong></summary>

```text
Master-Replica 구조로 구성했습니다. 
Master 노드는 읽기/쓰기를 모두 처리하고, Replica 노드는 읽기만 처리하도록 했습니다.
Master 노드에 문제가 생기면 Replica가 Master로 승격되어 서비스가 중단되지 않도록 구성했습니다.
```

</details>

---

<details>
<summary><strong style="font-size:1.17em">
Master 노드가 다운되면 어떻게 처리되나요?
</strong></summary>

```text
Sentinel을 도입하여 자동 페일오버가 이뤄지도록 했습니다.
Sentinel은 Master 노드의 헬스체크를 수행하다가 장애를 감지하면,
Replica 중에서 새로운 Master를 선출하고 클라이언트에게 새로운 Master 정보를 전달합니다.
```

</details>

---

<details>
<summary><strong style="font-size:1.17em">
리플리카에 대해 데이터 정합성은 어떻게 보장하셨나요?
</strong></summary>

```text
Master와 Replica 간 동기화는 기본적으로 비동기 방식으로 이뤄집니다.
하지만 이슈 번호의 경우 30초마다 DB에 동기화되고, 실제 서비스에서는 가공된 이슈 번호를 사용하기 때문에
일시적인 데이터 불일치가 서비스에 영향을 주지 않는 구조입니다.
```

</details>

---

<details>
<summary><strong style="font-size:1.17em">
왜 Replication만으로 구성하셨나요? Cluster는 고려하지 않으셨나요?
</strong></summary>

```text
이슈 번호 생성이라는 단순한 카운터 용도였고, 데이터 크기도 크지 않아 
Replication만으로도 충분한 가용성을 확보할 수 있었습니다.

Cluster는 복제및 스케일아웃의 장점이 있지만, 
구성이 복잡하고 관리 포인트가 늘어나는데 비해 
현재 요구사항에서는 그만한 이점이 없다고 판단했습니다.
```

</details>

---

<details>
<summary><strong style="font-size:1.17em">
Replica 노드는 몇 개를 구성하셨나요?
</strong></summary>

```text
Replica는 2대를 구성했습니다. 
한 대는 즉시 Master 승격이 가능하도록 하고, 
다른 한 대는 데이터 백업용으로 활용했습니다.
비용 대비 안정성을 고려했을 때 2대가 적절하다고 판단했습니다.
```

</details>

---

<details>
<summary><strong style="font-size:1.17em">
Redis 장애 상황을 실제로 겪어보셨나요?
</strong></summary>

```text
개발 환경에서 장애 시나리오를 테스트해봤습니다.
Master 노드를 강제로 중단시켜 페일오버를 테스트했고,
평균적으로 3초 이내에 서비스가 정상화되는 것을 확인했습니다.
```

</details>

---


## 동시성 문제

---

<details>
<summary><strong style="font-size:1.17em">
트러블 슈팅 경험이 있나요? 어떤 문제였고 어떻게 해결했나요? 
</strong></summary>

```text
저는 이전 프로젝트에서 이슈 생성 API를 개발하던 중 심각한 문제를 발견했습니다. 
여러 사용자가 동시에 이슈를 생성할 때 같은 이슈 번호(예: PROJECT-1)가 중복 생성되는 현상이 발생했습니다.

먼저 문제의 근본 원인을 분석했습니다. 
서로 다른 서비스에서 동시에 이슈 번호를 생성하는 과정에서 race condition이 발생한다는 것을 확인했습니다.

해결을 위해 다음과 같은 단계적인 접근을 시도했습니다

먼저 애플리케이션 레벨의 동시성 제어(synchronized, ReentrantLock)를 검토했으나, 
다중 서버 환경에서는 적합하지 않았습니다.

데이터베이스 레벨의 동시성 제어 방안을 검토했고, 최종적으로 
이슈 번호 관리를 위한 별도 테이블을 분리하고,
비관적 락을 적용하며, 독립된 트랜잭션으로 처리를 했습니다.

이후 이슈 생성 API의 성능 테스트를 통해 문제가 해결되었음을 확인했습니다.

```

</details>

---

<details>
<summary><strong style="font-size:1.17em">
왜 비관적 락을 선택했나요? 낙관적 락과의 차이점은 무엇인가요?
</strong></summary>

```text
비관적 락을 선택한 주된 이유는 두 가지입니다.

첫번째로, 낙관적 락의 경우 충돌 발생 시 재시도 로직이 필요한데, 
이는 구현 복잡도를 높이고 성능에도 영향을 줄 수 있습니다.

둘째로, 이슈 번호 생성은 단일 레코드에 대한 짧은 트랜잭션이기 때문에,
비관적 락을 사용하더라도 성능 저하가 크지 않습니다.
실제 성능 테스트 결과, 낙관적 락(800ms)보다 비관적 락(550ms)이 더 빠른 처리 속도를 보여주었습니다.

비관적 락의 주된 단점인 데드락 문제는
이슈 번호 생성 로직을 별도 테이블로 분리하고,
독립된 트랜잭션(REQUIRES_NEW)으로 처리함으로써 최소화했습니다.
이렇게 하면 다른 트랜잭션과의 상호작용을 줄여 데드락 발생 가능성을 낮출 수 있습니다.
```

</details>

---

<details>
<summary><strong style="font-size:1.17em">
성능 개선 효과를 어떻게 측정하셨나요?
</strong></summary>

```text
성능 테스트는 실제 운영 환경과 유사한 조건에서 진행했습니다.

테스트환경은 10명의 동시 사용자를 가정하고,
각 사용자가 1회씩 이슈 생성 메소드를 호출하는 시나리오를 구성했습니다.

테스트의 구성은 자바의 ForkJoinPool을 사용하고,
여러 스레드로 동시에 이슈 생성 메소드를 호출하도록 했습니다.
CountDownLatch를 사용해 모든 요청이 동시에 시작되도록 제어했습니다.

성능 측정 결과 낙관적락 적용 시 충돌 발생으로 재시도 횟수가 많아 처리 시간이 800ms가 소요되었으나,
비관적락 적용 시 550ms로 성능이 개선되었음을 확인했습니다.
```

</details>

---

## HikariCP 관련 트러블 슈팅

---

<details>
<summary><strong style="font-size:1.17em">
트러블 슈팅 경험이 있나요? 어떤 문제였고 어떻게 해결했나요? 
</strong></summary>

```text
1000명 규모의 조직을 대상으로 가정해, 이슈 생성 API의 성능 테스트를 진행하던 중이었습니다.
목표는 피크 타임을 고려해 동시사용자수는 100명, 30-40 TPS, 응답시간 2초 이내, 에러율 0.1% 이하였습니다.

Ngrinder를 사용한 성능 테스트 과정에서, VUser 10명일 때는 정상 동작했으나,
VUser 20명으로 증가시켰을 때 "Could not open JPA EntityManager for Transaction" 에러가 발생하고
request timeout에 도달하는 문제를 해결해야 했습니다.

먼저 문제의 원인을 분석했습니다:
HikariCP의 기본 커넥션 풀 크기가 10개로 설정되어 있었고
이슈 생성 요청당 2개의 커넥션이 필요한 상황이었습니다.
결과적으로 20개의 동시 요청을 처리하기에는 커넥션이 부족했습니다.

해결을 위해 
HikariCP의 커넥션 풀 크기를 20개로 늘리고, 최소 커넥션 수를 20개로 설정하여 
커넥션을 항상 유지하도록 했습니다.
커넥션 타임아웃은 3초, max-lifetime은 50초로 설정하여 MySQL의 wait_timeout보다 짧게 조정했습니다.
불필요한 트랜잭션 범위를 줄이고 REQUIRES_NEW를 제거하여 커넥션 사용을 최적화했습니다.

개선 후 
VUser 50 환경에서도 안정적으로 동작하고,
목표했던 30-40 TPS를 달성
응답 시간 2초 이내 유지
에러율 0.1% 이내를 달성했습니다.
```

</details>

---

<details>
<summary><strong style="font-size:1.17em">
VUser 20일 때, 타임아웃 에러가 발생한 이유는 무엇이라고 생각하나요? 
</strong></summary>

```text
초기에는 커넥션 풀 부족 문제로 추정했습니다.
요청 당 2개의 커넥션이 필요한 상황에서, 
HikariCP의 커넥션 풀 크기가 10개로 설정되어 있었기 때문입니다.

하지만 정확한 원인 파악을 위해 다음과 같은 모니터링을 진행했습니다:
//1. DB 커넥션 사용량 추이 확인
//2. 스레드 덤프를 통한 병목 구간 분석
3. show engine innodb status로 데드락 상태 확인
//4. 트랜잭션 처리 시간 모니터링

모니터링 결과, REQUIRES_NEW를 사용한 중첩 트랜잭션 구조에서
데드락이 발생하는 것을 확인했습니다.
이로 인해 트랜잭션이 장시간 대기 상태에 머물다가 
결국 timeout으로 이어지는 것이었습니다.

이를 해결하기 위해:
1. REQUIRES_NEW를 제거하여 트랜잭션 구조를 단순화
2. HikariCP의 connection-timeout을 3초로 설정
3. maximum-pool-size를 20으로 증가

그 결과 VUser 20 환경에서도 안정적으로 동작하는 것을 확인했습니다.
```

</details>
